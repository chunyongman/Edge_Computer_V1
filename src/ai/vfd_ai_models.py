"""
VFD AI ì˜ˆë°©ì§„ë‹¨ ëª¨ë¸
- Isolation Forest: ì´ìƒ íƒì§€
- LSTM: ì˜¨ë„ ì˜ˆì¸¡
- Random Forest: ê³ ì¥ ìœ í˜• ë¶„ë¥˜
"""

import logging
import pickle
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from datetime import datetime
from collections import deque

# scikit-learn
from sklearn.ensemble import IsolationForest, RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# TensorFlow/Keras (ê²½ëŸ‰ LSTM)
try:
    import tensorflow as tf
    from tensorflow.keras.models import Sequential, load_model
    from tensorflow.keras.layers import LSTM, Dense, Dropout
    from tensorflow.keras.callbacks import EarlyStopping
    TF_AVAILABLE = True
except ImportError:
    TF_AVAILABLE = False
    logging.warning("âš ï¸ TensorFlow ë¯¸ì„¤ì¹˜ - LSTM ê¸°ëŠ¥ ë¹„í™œì„±í™”")

from src.database.db_manager import get_db_manager

logger = logging.getLogger(__name__)


class IsolationForestAnomalyDetector:
    """
    Isolation Forest ê¸°ë°˜ ì´ìƒ íƒì§€

    ì •ìƒ ë°ì´í„° íŒ¨í„´ì„ í•™ìŠµí•˜ê³ , ì´ìƒì¹˜ë¥¼ íƒì§€
    """

    def __init__(self, model_dir: str = "models"):
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(parents=True, exist_ok=True)

        self.model: Optional[IsolationForest] = None
        self.scaler: Optional[StandardScaler] = None
        self.is_trained = False

        # ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
        self.model_path = self.model_dir / "isolation_forest.pkl"
        self.scaler_path = self.model_dir / "if_scaler.pkl"

        # ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ ì‹œë„
        self._load_model()

    def _load_model(self):
        """ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ"""
        try:
            if self.model_path.exists() and self.scaler_path.exists():
                with open(self.model_path, 'rb') as f:
                    self.model = pickle.load(f)
                with open(self.scaler_path, 'rb') as f:
                    self.scaler = pickle.load(f)
                self.is_trained = True
                logger.info("âœ… Isolation Forest ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"âš ï¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

    def train(self, features: np.ndarray, contamination: float = 0.1):
        """
        ëª¨ë¸ í•™ìŠµ

        Args:
            features: íŠ¹ì„± ë°ì´í„° (n_samples, n_features)
            contamination: ì˜ˆìƒ ì´ìƒì¹˜ ë¹„ìœ¨
        """
        if len(features) < 100:
            logger.warning("âš ï¸ í•™ìŠµ ë°ì´í„° ë¶€ì¡± (ìµœì†Œ 100ê°œ í•„ìš”)")
            return False

        try:
            # ìŠ¤ì¼€ì¼ë§
            self.scaler = StandardScaler()
            scaled_features = self.scaler.fit_transform(features)

            # Isolation Forest í•™ìŠµ
            self.model = IsolationForest(
                n_estimators=100,
                contamination=contamination,
                random_state=42,
                n_jobs=-1
            )
            self.model.fit(scaled_features)

            # ëª¨ë¸ ì €ì¥
            with open(self.model_path, 'wb') as f:
                pickle.dump(self.model, f)
            with open(self.scaler_path, 'wb') as f:
                pickle.dump(self.scaler, f)

            self.is_trained = True
            logger.info(f"âœ… Isolation Forest í•™ìŠµ ì™„ë£Œ: {len(features)}ê°œ ìƒ˜í”Œ")

            # ë©”íƒ€ë°ì´í„° ì €ì¥
            db = get_db_manager()
            db.save_model_metadata(
                model_name="isolation_forest",
                model_type="anomaly_detection",
                version="1.0",
                accuracy=0.0,  # ë¹„ì§€ë„ í•™ìŠµì´ë¼ ì •í™•ë„ ì—†ìŒ
                parameters={"contamination": contamination, "n_estimators": 100},
                file_path=str(self.model_path)
            )

            return True

        except Exception as e:
            logger.error(f"âŒ í•™ìŠµ ì‹¤íŒ¨: {e}")
            return False

    def predict(self, features: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        ì´ìƒ íƒì§€ ì˜ˆì¸¡

        Args:
            features: íŠ¹ì„± ë°ì´í„°

        Returns:
            (is_anomaly, anomaly_scores)
            - is_anomaly: Trueë©´ ì´ìƒ, Falseë©´ ì •ìƒ
            - anomaly_scores: ì´ìƒ ì ìˆ˜ (0-100, ë†’ì„ìˆ˜ë¡ ì´ìƒ)
        """
        if not self.is_trained:
            # í•™ìŠµë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ê°’ ë°˜í™˜
            return np.zeros(len(features), dtype=bool), np.zeros(len(features))

        try:
            scaled_features = self.scaler.transform(features)

            # -1: ì´ìƒ, 1: ì •ìƒ
            predictions = self.model.predict(scaled_features)
            is_anomaly = predictions == -1

            # ì´ìƒ ì ìˆ˜ (decision_function: ë‚®ì„ìˆ˜ë¡ ì´ìƒ)
            raw_scores = self.model.decision_function(scaled_features)
            # 0-100 ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜ (ë‚®ì€ ê°’ â†’ ë†’ì€ ì´ìƒ ì ìˆ˜)
            anomaly_scores = np.clip((0.5 - raw_scores) * 100, 0, 100)

            return is_anomaly, anomaly_scores

        except Exception as e:
            logger.error(f"âŒ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return np.zeros(len(features), dtype=bool), np.zeros(len(features))

    def predict_single(self, features: List[float]) -> Tuple[bool, float]:
        """ë‹¨ì¼ ìƒ˜í”Œ ì˜ˆì¸¡"""
        features_array = np.array([features])
        is_anomaly, scores = self.predict(features_array)
        return bool(is_anomaly[0]), float(scores[0])


class LSTMTemperaturePredictor:
    """
    LSTM ê¸°ë°˜ ì˜¨ë„ ì˜ˆì¸¡

    ì‹œê³„ì—´ ì˜¨ë„ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ì—¬ ë¯¸ë˜ ì˜¨ë„ ì˜ˆì¸¡
    """

    def __init__(self, model_dir: str = "models", sequence_length: int = 30):
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(parents=True, exist_ok=True)

        self.sequence_length = sequence_length
        self.model = None
        self.scaler: Optional[StandardScaler] = None
        self.is_trained = False

        self.model_path = self.model_dir / "lstm_temp.h5"
        self.scaler_path = self.model_dir / "lstm_scaler.pkl"

        if TF_AVAILABLE:
            self._load_model()

    def _load_model(self):
        """ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ"""
        if not TF_AVAILABLE:
            return

        try:
            if self.model_path.exists() and self.scaler_path.exists():
                self.model = load_model(str(self.model_path))
                with open(self.scaler_path, 'rb') as f:
                    self.scaler = pickle.load(f)
                self.is_trained = True
                logger.info("âœ… LSTM ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"âš ï¸ LSTM ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

    def _build_model(self, n_features: int):
        """LSTM ëª¨ë¸ êµ¬ì¶•"""
        model = Sequential([
            LSTM(64, input_shape=(self.sequence_length, n_features), return_sequences=True),
            Dropout(0.2),
            LSTM(32, return_sequences=False),
            Dropout(0.2),
            Dense(16, activation='relu'),
            Dense(1)  # ì˜¨ë„ ì˜ˆì¸¡
        ])
        model.compile(optimizer='adam', loss='mse', metrics=['mae'])
        return model

    def prepare_sequences(
        self,
        data: np.ndarray,
        target_col: int = 0
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        ì‹œê³„ì—´ ë°ì´í„°ë¥¼ LSTM ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜

        Args:
            data: (n_samples, n_features) ë°ì´í„°
            target_col: ì˜ˆì¸¡í•  ì»¬ëŸ¼ ì¸ë±ìŠ¤ (ê¸°ë³¸: ì˜¨ë„)

        Returns:
            X: (n_sequences, sequence_length, n_features)
            y: (n_sequences,) íƒ€ê²Ÿ ê°’
        """
        X, y = [], []

        for i in range(len(data) - self.sequence_length):
            X.append(data[i:i + self.sequence_length])
            y.append(data[i + self.sequence_length, target_col])

        return np.array(X), np.array(y)

    def train(
        self,
        temperature_data: np.ndarray,
        additional_features: np.ndarray = None,
        epochs: int = 50,
        batch_size: int = 32
    ):
        """
        LSTM ëª¨ë¸ í•™ìŠµ

        Args:
            temperature_data: ì˜¨ë„ ì‹œê³„ì—´ ë°ì´í„° (n_samples,)
            additional_features: ì¶”ê°€ íŠ¹ì„± (n_samples, n_features)
            epochs: í•™ìŠµ ì—í­
            batch_size: ë°°ì¹˜ í¬ê¸°
        """
        if not TF_AVAILABLE:
            logger.warning("âš ï¸ TensorFlow ë¯¸ì„¤ì¹˜ - LSTM í•™ìŠµ ë¶ˆê°€")
            return False

        if len(temperature_data) < self.sequence_length * 2:
            logger.warning("âš ï¸ í•™ìŠµ ë°ì´í„° ë¶€ì¡±")
            return False

        try:
            # ë°ì´í„° ì¤€ë¹„
            if additional_features is not None:
                data = np.column_stack([temperature_data, additional_features])
            else:
                data = temperature_data.reshape(-1, 1)

            # ìŠ¤ì¼€ì¼ë§
            self.scaler = StandardScaler()
            scaled_data = self.scaler.fit_transform(data)

            # ì‹œí€€ìŠ¤ ìƒì„±
            X, y = self.prepare_sequences(scaled_data, target_col=0)

            if len(X) < 50:
                logger.warning("âš ï¸ ì‹œí€€ìŠ¤ ë°ì´í„° ë¶€ì¡±")
                return False

            # Train/Test ë¶„í• 
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42
            )

            # ëª¨ë¸ êµ¬ì¶• ë° í•™ìŠµ
            n_features = X.shape[2]
            self.model = self._build_model(n_features)

            early_stop = EarlyStopping(
                monitor='val_loss',
                patience=10,
                restore_best_weights=True
            )

            self.model.fit(
                X_train, y_train,
                validation_data=(X_test, y_test),
                epochs=epochs,
                batch_size=batch_size,
                callbacks=[early_stop],
                verbose=0
            )

            # í‰ê°€
            loss, mae = self.model.evaluate(X_test, y_test, verbose=0)
            logger.info(f"âœ… LSTM í•™ìŠµ ì™„ë£Œ: MAE={mae:.4f}")

            # ëª¨ë¸ ì €ì¥
            self.model.save(str(self.model_path))
            with open(self.scaler_path, 'wb') as f:
                pickle.dump(self.scaler, f)

            self.is_trained = True

            # ë©”íƒ€ë°ì´í„° ì €ì¥
            db = get_db_manager()
            db.save_model_metadata(
                model_name="lstm_temperature",
                model_type="regression",
                version="1.0",
                accuracy=1.0 - mae,  # MAEë¥¼ ì •í™•ë„ë¡œ ë³€í™˜
                parameters={
                    "sequence_length": self.sequence_length,
                    "epochs": epochs
                },
                file_path=str(self.model_path)
            )

            return True

        except Exception as e:
            logger.error(f"âŒ LSTM í•™ìŠµ ì‹¤íŒ¨: {e}")
            return False

    def predict(
        self,
        recent_temps: np.ndarray,
        additional_features: np.ndarray = None,
        steps_ahead: int = 15
    ) -> List[float]:
        """
        ë¯¸ë˜ ì˜¨ë„ ì˜ˆì¸¡

        Args:
            recent_temps: ìµœê·¼ ì˜¨ë„ ë°ì´í„° (sequence_length,)
            additional_features: ì¶”ê°€ íŠ¹ì„±
            steps_ahead: ì˜ˆì¸¡í•  ìŠ¤í… ìˆ˜ (ê° ìŠ¤í… = 2ì´ˆ, 15ìŠ¤í… = 30ì´ˆ)

        Returns:
            ì˜ˆì¸¡ ì˜¨ë„ ë¦¬ìŠ¤íŠ¸
        """
        if not self.is_trained or not TF_AVAILABLE:
            # ì„ í˜• ì¶”ì„¸ë¡œ ê°„ë‹¨ ì˜ˆì¸¡
            if len(recent_temps) >= 2:
                slope = (recent_temps[-1] - recent_temps[0]) / len(recent_temps)
                return [recent_temps[-1] + slope * i for i in range(1, steps_ahead + 1)]
            return [recent_temps[-1]] * steps_ahead if len(recent_temps) > 0 else [0.0] * steps_ahead

        try:
            # ë°ì´í„° ì¤€ë¹„
            if additional_features is not None:
                data = np.column_stack([recent_temps[-self.sequence_length:], additional_features[-self.sequence_length:]])
            else:
                data = recent_temps[-self.sequence_length:].reshape(-1, 1)

            scaled_data = self.scaler.transform(data)

            predictions = []
            current_sequence = scaled_data.copy()

            for _ in range(steps_ahead):
                # ì˜ˆì¸¡
                X = current_sequence.reshape(1, self.sequence_length, -1)
                pred = self.model.predict(X, verbose=0)[0, 0]
                predictions.append(pred)

                # ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸
                new_row = current_sequence[-1].copy()
                new_row[0] = pred
                current_sequence = np.vstack([current_sequence[1:], new_row])

            # ì—­ìŠ¤ì¼€ì¼ë§ (ì˜¨ë„ë§Œ)
            dummy = np.zeros((len(predictions), self.scaler.n_features_in_))
            dummy[:, 0] = predictions
            unscaled = self.scaler.inverse_transform(dummy)

            return unscaled[:, 0].tolist()

        except Exception as e:
            logger.error(f"âŒ LSTM ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return [recent_temps[-1]] * steps_ahead if len(recent_temps) > 0 else [0.0] * steps_ahead

    def predict_30min(self, recent_temps: np.ndarray) -> float:
        """30ë¶„ í›„ ì˜¨ë„ ì˜ˆì¸¡"""
        # 2ì´ˆ ì£¼ê¸° ê¸°ì¤€: 30ë¶„ = 900ì´ˆ = 450 ìŠ¤í…
        # ê°„ëµí™”: 15ìŠ¤í…ì”© 30ë²ˆ ì˜ˆì¸¡
        predictions = self.predict(recent_temps, steps_ahead=450)
        return predictions[-1] if predictions else recent_temps[-1]


class RandomForestFaultClassifier:
    """
    Random Forest ê¸°ë°˜ ê³ ì¥ ìœ í˜• ë¶„ë¥˜

    ì´ìƒ íŒ¨í„´ì„ íŠ¹ì • ê³ ì¥ ìœ í˜•ìœ¼ë¡œ ë¶„ë¥˜
    """

    FAULT_TYPES = [
        "normal",           # ì •ìƒ
        "bearing_wear",     # ë² ì–´ë§ ë§ˆëª¨
        "cooling_fault",    # ëƒ‰ê° ì´ìƒ
        "electrical_fault", # ì „ê¸°ì  ì´ìƒ
        "overload",         # ê³¼ë¶€í•˜
        "vibration_fault"   # ì§„ë™ ì´ìƒ
    ]

    def __init__(self, model_dir: str = "models"):
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(parents=True, exist_ok=True)

        self.model: Optional[RandomForestClassifier] = None
        self.scaler: Optional[StandardScaler] = None
        self.is_trained = False

        self.model_path = self.model_dir / "random_forest_fault.pkl"
        self.scaler_path = self.model_dir / "rf_scaler.pkl"

        self._load_model()

    def _load_model(self):
        """ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ"""
        try:
            if self.model_path.exists() and self.scaler_path.exists():
                with open(self.model_path, 'rb') as f:
                    self.model = pickle.load(f)
                with open(self.scaler_path, 'rb') as f:
                    self.scaler = pickle.load(f)
                self.is_trained = True
                logger.info("âœ… Random Forest ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"âš ï¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

    def train(self, features: np.ndarray, labels: np.ndarray):
        """
        ëª¨ë¸ í•™ìŠµ

        Args:
            features: íŠ¹ì„± ë°ì´í„°
            labels: ê³ ì¥ ìœ í˜• ë¼ë²¨
        """
        if len(features) < 50:
            logger.warning("âš ï¸ í•™ìŠµ ë°ì´í„° ë¶€ì¡±")
            return False

        try:
            # ìŠ¤ì¼€ì¼ë§
            self.scaler = StandardScaler()
            scaled_features = self.scaler.fit_transform(features)

            # Train/Test ë¶„í• 
            X_train, X_test, y_train, y_test = train_test_split(
                scaled_features, labels, test_size=0.2, random_state=42
            )

            # Random Forest í•™ìŠµ
            self.model = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                random_state=42,
                n_jobs=-1
            )
            self.model.fit(X_train, y_train)

            # í‰ê°€
            accuracy = self.model.score(X_test, y_test)
            logger.info(f"âœ… Random Forest í•™ìŠµ ì™„ë£Œ: ì •í™•ë„={accuracy:.4f}")

            # ëª¨ë¸ ì €ì¥
            with open(self.model_path, 'wb') as f:
                pickle.dump(self.model, f)
            with open(self.scaler_path, 'wb') as f:
                pickle.dump(self.scaler, f)

            self.is_trained = True

            # ë©”íƒ€ë°ì´í„° ì €ì¥
            db = get_db_manager()
            db.save_model_metadata(
                model_name="random_forest_fault",
                model_type="classification",
                version="1.0",
                accuracy=accuracy,
                parameters={"n_estimators": 100, "max_depth": 10},
                file_path=str(self.model_path)
            )

            return True

        except Exception as e:
            logger.error(f"âŒ í•™ìŠµ ì‹¤íŒ¨: {e}")
            return False

    def predict(self, features: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        ê³ ì¥ ìœ í˜• ì˜ˆì¸¡

        Args:
            features: íŠ¹ì„± ë°ì´í„°

        Returns:
            (predicted_labels, probabilities)
        """
        if not self.is_trained:
            return np.array(["normal"] * len(features)), np.zeros((len(features), len(self.FAULT_TYPES)))

        try:
            scaled_features = self.scaler.transform(features)
            predictions = self.model.predict(scaled_features)
            probabilities = self.model.predict_proba(scaled_features)

            return predictions, probabilities

        except Exception as e:
            logger.error(f"âŒ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return np.array(["normal"] * len(features)), np.zeros((len(features), len(self.FAULT_TYPES)))

    def predict_single(self, features: List[float]) -> Dict:
        """
        ë‹¨ì¼ ìƒ˜í”Œ ê³ ì¥ ìœ í˜• ì˜ˆì¸¡

        Returns:
            {
                "predicted_fault": "bearing_wear",
                "confidence": 0.85,
                "probabilities": {"normal": 0.1, "bearing_wear": 0.85, ...}
            }
        """
        features_array = np.array([features])
        predictions, probabilities = self.predict(features_array)

        prob_dict = {}
        if self.is_trained and hasattr(self.model, 'classes_'):
            for i, cls in enumerate(self.model.classes_):
                prob_dict[cls] = float(probabilities[0, i])
        else:
            prob_dict = {ft: 0.0 for ft in self.FAULT_TYPES}
            prob_dict["normal"] = 1.0

        return {
            "predicted_fault": predictions[0],
            "confidence": float(np.max(probabilities[0])) if len(probabilities[0]) > 0 else 1.0,
            "probabilities": prob_dict
        }

    def generate_synthetic_training_data(self, n_samples: int = 1000) -> Tuple[np.ndarray, np.ndarray]:
        """
        í•©ì„± í•™ìŠµ ë°ì´í„° ìƒì„± (ì‹¤ì œ ê³ ì¥ ë°ì´í„° ì—†ì„ ë•Œ ì‚¬ìš©)

        ì‹¤ì œ ìš´ì˜ì—ì„œëŠ” ì‹¤ì œ ê³ ì¥ ì´ë ¥ ë°ì´í„°ë¡œ í•™ìŠµí•´ì•¼ í•¨
        """
        np.random.seed(42)

        features_list = []
        labels_list = []

        samples_per_class = n_samples // len(self.FAULT_TYPES)

        for fault_type in self.FAULT_TYPES:
            for _ in range(samples_per_class):
                # ê¸°ë³¸ ì •ìƒ íŒ¨í„´
                features = [
                    np.random.normal(65, 5),   # ì˜¨ë„ í‰ê· 
                    np.random.normal(2, 0.5),  # ì˜¨ë„ í‘œì¤€í¸ì°¨
                    np.random.normal(72, 5),   # ì˜¨ë„ ìµœëŒ€
                    np.random.normal(58, 5),   # ì˜¨ë„ ìµœì†Œ
                    np.random.normal(0, 0.1),  # ì˜¨ë„ ì¶”ì„¸
                    np.random.normal(0, 2),    # ì˜¨ë„ ë³€í™”ëŸ‰
                    np.random.normal(55, 5),   # íˆíŠ¸ì‹±í¬ ì˜¨ë„
                    np.random.normal(2, 0.5),  # íˆíŠ¸ì‹±í¬ í‘œì¤€í¸ì°¨
                    np.random.normal(62, 5),   # íˆíŠ¸ì‹±í¬ ìµœëŒ€
                    np.random.normal(55, 5),   # íˆíŠ¸ì‹±í¬ í˜„ì¬
                    np.random.normal(100, 20), # ì „ë¥˜ í‰ê· 
                    np.random.normal(10, 3),   # ì „ë¥˜ í‘œì¤€í¸ì°¨
                    np.random.normal(120, 20), # ì „ë¥˜ ìµœëŒ€
                    np.random.normal(100, 20), # ì „ë¥˜ í˜„ì¬
                    np.random.normal(45, 5),   # ì£¼íŒŒìˆ˜ í‰ê· 
                    np.random.normal(2, 0.5),  # ì£¼íŒŒìˆ˜ í‘œì¤€í¸ì°¨
                    np.random.normal(45, 5),   # ì£¼íŒŒìˆ˜ í˜„ì¬
                    np.random.normal(10, 5),   # ì‹¬ê°ë„ í‰ê· 
                    np.random.normal(15, 5),   # ì‹¬ê°ë„ ìµœëŒ€
                    np.random.normal(10, 5),   # ì‹¬ê°ë„ í˜„ì¬
                ]

                # ê³ ì¥ ìœ í˜•ë³„ íŒ¨í„´ ë³€ì¡°
                if fault_type == "bearing_wear":
                    features[0] += 10  # ì˜¨ë„ ìƒìŠ¹
                    features[4] += 0.3  # ì˜¨ë„ ì¶”ì„¸ ìƒìŠ¹
                    features[10] += 20  # ì „ë¥˜ ì¦ê°€

                elif fault_type == "cooling_fault":
                    features[0] += 15  # ì˜¨ë„ ê¸‰ìƒìŠ¹
                    features[6] += 15  # íˆíŠ¸ì‹±í¬ ì˜¨ë„ ìƒìŠ¹
                    features[4] += 0.5  # ì˜¨ë„ ì¶”ì„¸ ê¸‰ìƒìŠ¹

                elif fault_type == "electrical_fault":
                    features[10] += 40  # ì „ë¥˜ ê¸‰ì¦
                    features[11] += 10  # ì „ë¥˜ ë³€ë™ ì¦ê°€
                    features[17] += 30  # ì‹¬ê°ë„ ì¦ê°€

                elif fault_type == "overload":
                    features[10] += 50  # ì „ë¥˜ ë§¤ìš° ë†’ìŒ
                    features[0] += 8   # ì˜¨ë„ ìƒìŠ¹
                    features[14] += 10  # ì£¼íŒŒìˆ˜ ì¦ê°€

                elif fault_type == "vibration_fault":
                    features[1] += 5   # ì˜¨ë„ ë³€ë™ ì¦ê°€
                    features[11] += 15  # ì „ë¥˜ ë³€ë™ ì¦ê°€
                    features[15] += 5   # ì£¼íŒŒìˆ˜ ë³€ë™

                features_list.append(features)
                labels_list.append(fault_type)

        return np.array(features_list), np.array(labels_list)


class VFDAIEngine:
    """
    VFD AI ì˜ˆë°©ì§„ë‹¨ í†µí•© ì—”ì§„

    ëª¨ë“  AI ëª¨ë¸ì„ í†µí•©í•˜ì—¬ ì¢…í•© ì§„ë‹¨ ì œê³µ
    """

    def __init__(self, model_dir: str = "models"):
        self.model_dir = Path(model_dir)

        # AI ëª¨ë¸ë“¤ ì´ˆê¸°í™”
        self.anomaly_detector = IsolationForestAnomalyDetector(str(self.model_dir))
        self.temp_predictor = LSTMTemperaturePredictor(str(self.model_dir))
        self.fault_classifier = RandomForestFaultClassifier(str(self.model_dir))

        # ë°ì´í„° ë²„í¼ (VFDë³„)
        self.data_buffers: Dict[str, deque] = {}
        self.buffer_size = 60  # 2ë¶„ (2ì´ˆ ì£¼ê¸°)

        logger.info("âœ… VFD AI Engine ì´ˆê¸°í™” ì™„ë£Œ")

    def add_data_point(
        self,
        vfd_id: str,
        motor_temp: float,
        heatsink_temp: float,
        current: float,
        frequency: float,
        severity_score: int
    ):
        """ë°ì´í„° í¬ì¸íŠ¸ ì¶”ê°€"""
        if vfd_id not in self.data_buffers:
            self.data_buffers[vfd_id] = deque(maxlen=self.buffer_size)

        self.data_buffers[vfd_id].append({
            'motor_temp': motor_temp,
            'heatsink_temp': heatsink_temp,
            'current': current,
            'frequency': frequency,
            'severity_score': severity_score,
            'timestamp': datetime.now()
        })

    def analyze(self, vfd_id: str) -> Dict:
        """
        ì¢…í•© AI ë¶„ì„

        Returns:
            {
                "anomaly_detected": bool,
                "anomaly_score": float (0-100),
                "predicted_temp_30min": float,
                "temp_trend": str,
                "fault_prediction": {
                    "predicted_fault": str,
                    "confidence": float,
                    "probabilities": dict
                },
                "recommendations": list,
                "risk_level": str  # "low", "medium", "high", "critical"
            }
        """
        result = {
            "anomaly_detected": False,
            "anomaly_score": 0.0,
            "predicted_temp_30min": 0.0,
            "temp_trend": "stable",
            "fault_prediction": {
                "predicted_fault": "normal",
                "confidence": 1.0,
                "probabilities": {}
            },
            "recommendations": [],
            "risk_level": "low"
        }

        buffer = self.data_buffers.get(vfd_id)
        if not buffer or len(buffer) < 30:
            result["recommendations"].append("ë°ì´í„° ìˆ˜ì§‘ ì¤‘... (ìµœì†Œ 1ë¶„ í•„ìš”)")
            return result

        data = list(buffer)

        # íŠ¹ì„± ì¶”ì¶œ
        features = self._extract_features(data)

        # 1. ì´ìƒ íƒì§€
        is_anomaly, anomaly_score = self.anomaly_detector.predict_single(features)
        result["anomaly_detected"] = is_anomaly
        result["anomaly_score"] = anomaly_score

        # 2. ì˜¨ë„ ì˜ˆì¸¡
        temps = np.array([d['motor_temp'] for d in data])
        predictions = self.temp_predictor.predict(temps, steps_ahead=900)  # 30ë¶„
        result["predicted_temp_30min"] = predictions[-1] if predictions else temps[-1]

        # ì˜¨ë„ ì¶”ì„¸
        if len(temps) >= 10:
            slope = np.polyfit(range(len(temps)), temps, 1)[0]
            if slope > 0.1:
                result["temp_trend"] = "rising"
            elif slope < -0.1:
                result["temp_trend"] = "falling"
            else:
                result["temp_trend"] = "stable"

        # 3. ê³ ì¥ ìœ í˜• ì˜ˆì¸¡
        fault_result = self.fault_classifier.predict_single(features)
        result["fault_prediction"] = fault_result

        # 4. ìœ„í—˜ë„ í‰ê°€ ë° ê¶Œê³ ì‚¬í•­ ìƒì„±
        result["risk_level"], result["recommendations"] = self._evaluate_risk(
            anomaly_score=anomaly_score,
            predicted_temp=result["predicted_temp_30min"],
            fault_type=fault_result["predicted_fault"],
            fault_confidence=fault_result["confidence"]
        )

        return result

    def _extract_features(self, data: List[Dict]) -> List[float]:
        """íŠ¹ì„± ë²¡í„° ì¶”ì¶œ"""
        motor_temps = [d['motor_temp'] for d in data]
        heatsink_temps = [d['heatsink_temp'] for d in data]
        currents = [d['current'] for d in data]
        frequencies = [d['frequency'] for d in data]
        severity_scores = [d['severity_score'] for d in data]

        x = np.arange(len(motor_temps))
        temp_slope = np.polyfit(x, motor_temps, 1)[0] if len(motor_temps) > 1 else 0

        return [
            np.mean(motor_temps),
            np.std(motor_temps),
            np.max(motor_temps),
            np.min(motor_temps),
            temp_slope,
            motor_temps[-1] - motor_temps[0],
            np.mean(heatsink_temps),
            np.std(heatsink_temps),
            np.max(heatsink_temps),
            heatsink_temps[-1],
            np.mean(currents),
            np.std(currents),
            np.max(currents),
            currents[-1],
            np.mean(frequencies),
            np.std(frequencies),
            frequencies[-1],
            np.mean(severity_scores),
            np.max(severity_scores),
            severity_scores[-1],
        ]

    def _evaluate_risk(
        self,
        anomaly_score: float,
        predicted_temp: float,
        fault_type: str,
        fault_confidence: float
    ) -> Tuple[str, List[str]]:
        """ìœ„í—˜ë„ í‰ê°€ ë° ê¶Œê³ ì‚¬í•­ ìƒì„±"""
        recommendations = []
        risk_score = 0

        # ì´ìƒ ì ìˆ˜ ê¸°ë°˜
        if anomaly_score > 70:
            risk_score += 40
            recommendations.append("âš ï¸ ë¹„ì •ìƒ íŒ¨í„´ ê°ì§€ - ì¦‰ì‹œ ì ê²€ í•„ìš”")
        elif anomaly_score > 50:
            risk_score += 25
            recommendations.append("âš ï¸ ì´ìƒ ì§•í›„ ê°ì§€ - ì£¼ì˜ ê´€ì°° í•„ìš”")
        elif anomaly_score > 30:
            risk_score += 10

        # ì˜ˆì¸¡ ì˜¨ë„ ê¸°ë°˜
        if predicted_temp > 85:
            risk_score += 35
            recommendations.append(f"ğŸŒ¡ï¸ 30ë¶„ í›„ ì˜¨ë„ {predicted_temp:.1f}Â°C ì˜ˆìƒ - ëƒ‰ê° ì¡°ì¹˜ í•„ìš”")
        elif predicted_temp > 75:
            risk_score += 20
            recommendations.append(f"ğŸŒ¡ï¸ 30ë¶„ í›„ ì˜¨ë„ {predicted_temp:.1f}Â°C ì˜ˆìƒ - ëª¨ë‹ˆí„°ë§ ê°•í™”")

        # ê³ ì¥ ìœ í˜• ê¸°ë°˜
        if fault_type != "normal" and fault_confidence > 0.7:
            risk_score += 25

            fault_recommendations = {
                "bearing_wear": "ğŸ”§ ë² ì–´ë§ ë§ˆëª¨ ì˜ì‹¬ - ì§„ë™ ì¸¡ì • ë° ìœ¤í™œ ìƒíƒœ ì ê²€ ê¶Œì¥",
                "cooling_fault": "â„ï¸ ëƒ‰ê° ì´ìƒ ì˜ì‹¬ - ëƒ‰ê°íŒ¬ ë° ë°©ì—´íŒ ì ê²€ ê¶Œì¥",
                "electrical_fault": "âš¡ ì „ê¸°ì  ì´ìƒ ì˜ì‹¬ - ì „ì› ë° ë°°ì„  ì ê²€ ê¶Œì¥",
                "overload": "ğŸ“ˆ ê³¼ë¶€í•˜ ìƒíƒœ - ë¶€í•˜ ê°ì†Œ ë˜ëŠ” ìš©ëŸ‰ ê²€í†  í•„ìš”",
                "vibration_fault": "ğŸ“³ ì§„ë™ ì´ìƒ ì˜ì‹¬ - ì„¤ì¹˜ ìƒíƒœ ë° ë°¸ëŸ°ì‹± ì ê²€ ê¶Œì¥"
            }

            if fault_type in fault_recommendations:
                recommendations.append(fault_recommendations[fault_type])
                recommendations.append(f"   (ì‹ ë¢°ë„: {fault_confidence * 100:.0f}%)")

        # ìœ„í—˜ë„ ë“±ê¸‰ ê²°ì •
        if risk_score >= 60:
            risk_level = "critical"
            if not any("ì¦‰ì‹œ" in r for r in recommendations):
                recommendations.insert(0, "ğŸš¨ ì¦‰ì‹œ ì ê²€ í•„ìš”")
        elif risk_score >= 40:
            risk_level = "high"
            recommendations.insert(0, "âš ï¸ 1ì£¼ì¼ ë‚´ ì ê²€ ê¶Œì¥")
        elif risk_score >= 20:
            risk_level = "medium"
            recommendations.insert(0, "ğŸ“‹ ì •ê¸° ì ê²€ ì‹œ í™•ì¸ í•„ìš”")
        else:
            risk_level = "low"
            if not recommendations:
                recommendations.append("âœ… ì •ìƒ ìš´ì „ ì¤‘")

        return risk_level, recommendations

    def train_models(self, training_data: Dict[str, np.ndarray] = None):
        """
        ëª¨ë“  ëª¨ë¸ í•™ìŠµ

        Args:
            training_data: {
                "features": np.ndarray,
                "temperatures": np.ndarray,
                "fault_labels": np.ndarray (optional)
            }
        """
        if training_data is None:
            logger.info("ğŸ”„ í•©ì„± ë°ì´í„°ë¡œ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")

            # Isolation Forest: ì •ìƒ ë°ì´í„° íŒ¨í„´ í•™ìŠµ
            # ì‹¤ì œë¡œëŠ” ìˆ˜ì§‘ëœ ì •ìƒ ë°ì´í„° ì‚¬ìš©
            normal_features = np.random.normal(0, 1, (500, 20))
            self.anomaly_detector.train(normal_features)

            # Random Forest: í•©ì„± ë°ì´í„°ë¡œ í•™ìŠµ
            features, labels = self.fault_classifier.generate_synthetic_training_data()
            self.fault_classifier.train(features, labels)

            # LSTM: ì˜¨ë„ ë°ì´í„° í•„ìš”
            if TF_AVAILABLE:
                temps = np.random.normal(65, 5, 1000)  # ìƒ˜í”Œ ë°ì´í„°
                self.temp_predictor.train(temps)

            logger.info("âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
        else:
            # ì‹¤ì œ ë°ì´í„°ë¡œ í•™ìŠµ
            if "features" in training_data:
                self.anomaly_detector.train(training_data["features"])

            if "temperatures" in training_data:
                self.temp_predictor.train(training_data["temperatures"])

            if "features" in training_data and "fault_labels" in training_data:
                self.fault_classifier.train(
                    training_data["features"],
                    training_data["fault_labels"]
                )


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_ai_engine: Optional[VFDAIEngine] = None


def get_ai_engine() -> VFDAIEngine:
    """VFDAIEngine ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _ai_engine
    if _ai_engine is None:
        _ai_engine = VFDAIEngine()
    return _ai_engine
